# Implementation Results
Raw data for implementation report.

## Result Data
Each implementation should record
which features they have implemented and tested under the `results` directory.
All data will be read and merged into the report.
Mark each implemented feature with a 1.
These will be summed for the final report.
Features not listed will be treated as unimplemented.
If you want to mention a feature but have not implemented it
record it with a zero.
If you have tested a feature in multiple
implementations either check in additional files.
Use a convention
like corporation-implementation.csv for the filename.
The filename should also be used as an id in the template
to describe each implementation.

The template.csv file lists all features but with implementation
counts of 0.
Do not edit this; it is autogenerated.
It is provided so
you can use it as a reference and as a basis for your own data files.

Files should be in CSV format, including headers as defined in template.csv,
and will be parsed by the csvtojson Node.js library.

## To Dos
1. Track tests of features that require two implementations to
interact.  Probably need to add a column that tracks the other
implementation's name (as given by the root name of these files).
2. Provide a way to assign assertions to categories and sort them
together in the output.
3. Provide an example implementation description template.
4. Track whether an implementation is a client, a server, or both.
5. Provide a list of additional "candidate" assertions to cover aspects
that are tested but not (yet) included in the actual specification.
These should be formatted in the report in a way that distinguishes them
from assertions already in the specification.
6. Internal hyperlinks in assertions should be removed or fixed (rebased).

